---
title: "MATH-517: Assignment 2"
author: "Filippo Reina"
date: 2025-10-05
format: pdf
editor: visual
---

# 1. Theoretical exercise: Local linear regression as a linear smoother

## Question

Recall our basic setup: we are given i.i.d. samples $\left(x_i, y_i\right), i=1, \ldots n$ from the model

$$y_i=m\left(x_i\right)+\epsilon_i, \quad i=1, \ldots n$$

and our goal is to estimate $m$ with some function $\hat{m}$. Assume that each $x_i \in \mathbb{R}$ (i.e., the predictors are 1-dimensional).

The local linear regression estimator at a point $x$ is defined by

$$(\hat\beta_0(x), \hat\beta_1(x)) 
= \arg\min_{\beta_0,\beta_1 \in \mathbb{R}} 
\sum_{i=1}^n \Big(Y_i - \beta_0 - \beta_1 (X_i - x)\Big)^2 
K\left(\frac{X_i - x}{h}\right),\tag{1}\label{eq:1}$$

where $K$ is a kernel function and $h>0$ is a bandwidth. The fitted value is then given by $\hat m(x) = \hat\beta_0(x)$ and we will show that such estimator belongs to the class of linear smoothers, so that

$$\hat{m}(x)=\sum_{i=1}^n w\left(x, x_i\right) \cdot y_i$$

for some choice of weights $w\left(x, x_i\right)$.

1.  Show that $\hat m(x)$ can be expressed as a weighted average of the observations:

    $$\hat m(x) = \sum_{i=1}^n w_{ni}(x) Y_i,$$

    where the weights $w_{ni}(x)$ depend only on $x$, $\{X_i\}$, $K$, and $h$, but not on the $Y_i$â€™s.

2.  Using the notation

    $$S_{n,k}(x) = \frac{1}{nh}\sum_{i=1}^n (X_i - x)^k K\left(\frac{X_i - x}{h}\right), \quad k=0,1,2,\tag{2}\label{eq:2}$$

    derive an explicit expression for $w_{ni}(x)$ in terms of $S_{n,0}(x), S_{n,1}(x), S_{n,2}(x)$, and the kernel.

3.  Prove that the weights satisfy $\sum_{i=1}^n w_{ni}(x) = 1$.

## Answer

1.  Looking at Equation \eqref{eq:1}, we can write $\hat{\beta}$ as the solution to a weighted least-squares problem: $$\hat{ \beta}=(\mathbf X^\top \mathbf W\mathbf X)^{-1}\mathbf X^\top\mathbf W \mathbf Y,$$ where we defined: $$\mathbf Y=(Y_1,\cdots, Y_n)^\top,\qquad \mathbf W=\operatorname{diag}(K_1,\cdots,K_n),\qquad \mathbf X=\begin{bmatrix}1&X_1-x\\\vdots&\vdots\\1&X_n-x
    \end{bmatrix}$$ and we denote $K_i=K\left((X_i-x)/h\right)$.

    Since $\hat m(x)=\hat \beta_0$ is linear, there exist weights independent of $Y_i$ such that: $$\hat m(x)=\sum_{i=1}^nw_{ni}(x)Y_i.$$

2.  We start by considering $\mathbf X^\top \mathbf W \mathbf X$. $$\mathbf X^\top \mathbf W \mathbf X=\mathbf X^\top \begin{bmatrix}
    K_1&K_1(X_1-x)\\
    \vdots & \vdots\\
    K_n&K_n(X_n-x)
    \end{bmatrix}=\begin{bmatrix}
    \sum_{i=1}^nK_i& \sum_{i=1}^nK_i(X_i-x)\\
    \sum_{i=1}^n(X_i-x)K_i&\sum_{i=1}^nK_i(X_i-x)^2
    \end{bmatrix}.$$ Looking at Equation \eqref{eq:2}, it follows that: $$\mathbf X^\top \mathbf W \mathbf X=nh\begin{bmatrix}
    S_{n,0}(x)&S_{n,1}(x)\\
    S_{n,1}(x)&S_{n,2}(x)
    \end{bmatrix}.$$ The determinant of that matrix is $S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2$, so the inverse is: $$(\mathbf X^\top \mathbf W \mathbf X)^{-1}=\frac{1}{nh\left[S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right]}\begin{bmatrix}
    S_{n,2}(x)&-S_{n,1}(x)\\
    -S_{n,1}(x)&S_{n,0}(x)
    \end{bmatrix}.$$ Proceding to $\mathbf X^\top \mathbf W$, what is simply a $2\times n$ matrix: $$\mathbf X^\top \mathbf W=\begin{bmatrix}
    K_1&\cdots&K_n\\
    (X_1-x)K_1&\cdots&(X_n-x) K_n
    \end{bmatrix}.$$ Notice that the first row of $(\mathbf X^\top \mathbf W \mathbf X)^{-1}\mathbf X^\top \mathbf W$ is the row vector $(w_{ni}(x))_{i=1,\cdots,n}.$

    Ignoring the normalizing constant, we can write the $i$th entry of $w_{ni}(x)$, $w_{ni}(x)=S_{n,2}(x)K_i-S_{n,1}(X_i-x)K_i$, and thus: $$w_{ni}(x)=\frac{K\left(\frac{X_i-x}{h}\right)\left(S_{n,2}(x)-(X_i-x)S_{n,1}(x)\right)}{nh\left[S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right]}.\tag{3}\label{eq:3}$$

3.  Looking at the Formula \eqref{eq:3}, it we need to show that $\sum_{i=1}^nw_{ni}(x)=1$. $$\sum_{i=1}^nw_{ni}(x)=\frac{1}{nh\left\{S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right\}}\left\{S_{n,2}(x)\underbrace{\sum_{i=1}^nK_i}_{(a)}-S_{n,1}(x)\underbrace{\sum_{i=1}^nK_i(X_i-x)}_{(b)}\right\},$$ where: $$(a)=nh\cdot S_{n,0}(x),\qquad (b)=nh\cdot S_{n,1}(x).$$ So, we can simplify the numerator to: $$nh\left\{S_{n,2}(x)S_{n,0}(x)-S_{n,1}(x)S_{n,1}(x)\right\}=nh\left\{S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right\},$$ and it follows that: $$\sum_{i=1}^nw_{ni}(x)=\frac{nh\left\{S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right\}}{nh\left\{S_{n,0}(x)S_{n,2}(x)-S_{n,1}(x)^2\right\}}=1.$$

\newpage

# 2. Practical exercise: Global bandwidth selection

Assume that we have a sample $\{(X_i,Y_i)\}_{i=1}^n$ of i.i.d. random vectors and that we are interested in estimating the conditional expectation $m(x) = {\mathbb{E}}(Y \mid X=x)$. We consider here the local linear estimator $\hat{m}$, as defined in Slide 14 of [Lecture 3](https://math-516-517-main.github.io/math_517_website/lectures/04_Smoothing.pdf).

Throughout the assignment, we will assume homoscedasticity, i.e., the local variance $\sigma^2(x) = {\mathbb{V}}(Y \mid X=x) \equiv \sigma^2$, as well as a quartic (biweight) kernel for $\hat{m}$. Under these assumptions, we know that the optimal bandwidth minimising the asymptotic mean integrated squared error is given by

$$h_{AMISE} = n^{-1/5} \bigg( \frac{35 \sigma^2 \vert supp(X) \vert}{\theta_{22}} \bigg)^{1/5}, \quad \theta_{22}= \int \lbrace m''(x) \rbrace^2 f_{X}(x) dx$$

where the two unknown quantities $\sigma^2$ and $\theta_{22}$ can be estimated by parametric OLS. For instance, one can

-   Block the sample in $N$ blocks and fit, in each block $j$, the model $$y_i = \beta_{0j} + \beta_{1j} x_i + \beta_{2j} x_i^2 + \beta_{3j} x_i^3 + \beta_{4j} x_i^4 + \epsilon_i$$ to obtain estimate $$\hat{m}_j = \hat{\beta}_{0j} + \hat{\beta}_{1j} x_i + \hat{\beta}_{2j} x_i^2 + \hat{\beta}_{3j} x_i^3 + \hat{\beta}_{4j} x_i^4$$

-   Estimate the unknown quantities by

$$\hat{\theta}_{22}(N) = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^N \hat{m}_j''(X_i) \hat{m}_j''(X_i)  \mathbb{1}_{X_i \in \mathcal{X}_j}$$

$$\hat{\sigma}^2(N) = \frac{1}{n-5N} \sum_{i=1}^n \sum_{j=1}^N \lbrace Y_i - \hat{m}_j(X_i) \rbrace^2 \mathbb{1}_{X_i \in \mathcal{X}_j}$$

## Task

The goal is to perform a simulation study to assess the impact of some parameters/hyperparameters on the optimal bandwidth $h_{AMISE}$. For instance, we will assume the following setting for the simulation study

-   a covariate $X$ from a beta distribution Beta $(\alpha,\beta)$
-   a response values $Y = m(X) + \epsilon$ where
    -   the regression function $m$ is given by $\sin\left\lbrace\left(\frac{x}{3}+0.1\right)^{-1}\right\rbrace$
    -   $\epsilon \sim \mathcal{N}(0,\sigma^2)$
-   fix $\sigma^2$ at some visually appealing value (e.g., $\sigma^2=1$ should be fine)

From there, estimate $h_{AMISE}$ as described above and in [Lecture 3](https://math-516-517-main.github.io/math_517_website/lectures/04_Smoothing.pdf) for different values of the following parameters/hyperparameters

-   the sample size $n$ (to assess the impact of the amount of available information),
-   the block size $N$ in the estimation of the unknown quantities $\sigma^2$ and $\theta_{22}$, and
-   the parameters $\alpha$ and $\beta$ of the beta density of the covariate (to assess the impact of the shape of the distribution of the covariate).

Comment and report your findings using appropriate visualisation tools. Possible questions to address:

-   How does $h_{AMISE}$ behave when $N$ grows? Can you explain why?
-   Should $N$ depend on $n$? Why?
-   What happens when the number of observations varies a lot between different regions in the support of $X$? How is this linked to the parameters of the Beta distribution?

When assessing the effect of the sample size $n$ or the density support of the covariate $X$ on the optimal bandwidth $h_{AMISE}$, you can fix the value of $N$ at an optimal value. This value could be considered as optimal in the sense that it minimizes the Mallow's $C_p$

$$ C_p(N)=\text{RSS}(N) / \lbrace \text{RSS} (N_{\max }) / (n-5 N_{\max })\rbrace -(n-10 N), $$

where

$$\text{RSS}(N) =  \sum_{i=1}^n \sum_{j=1}^N \lbrace Y_i - \hat{m}_j(X_i) \rbrace^2 \mathbb{1}_{X_i \in \mathcal{X}_j}$$

and $N_{\max}= \max \lbrace \min (\lfloor n / 20\rfloor, 5 ), 1\rbrace$; see [Ruppert et al. (1995)](https://sites.stat.washington.edu/courses/stat527/s13/readings/Ruppert_etal_JASA_1995.pdf) for choosing the optimal block size.

\newpage

## Results

Looking at @fig-bandwidth, we can see that as $N$ increases, $h_{AMISE}$ decreases.

![XXX](bandwidth_vs_cp_side_by_side.png){#fig-bandwidth width=500}

Intuitively, with very few blocks the quartic fits are forced to cover large ranges of $x$, which leads to an underestimation of $\hat \theta_{22}$, pushing $\hat h$ upwards. As $N$ increases, the fitted model tracks better the local curvature, leading to higher $\hat\theta_{22}$ and lower $\hat \sigma^2$, which translate to lower $\hat h$. This trend is captured by @fig-theta_sigma, that shows that our estimate $\hat\sigma$ is independent of $N$, while $\hat\theta_{22}$ drops when $N$ increases.

![XXX](sigma_theta_vs_N_side_by_side.png){#fig-theta_sigma width=500}

@fig-bandwidth also shows that, all else being equal, larger sample sizes $n$ lead to a lower $h_{AMISE}$. Looking at the right panel, it seems that the optimal value number of bins $N$ might depend on $n$. To analyze this further, we plot the optimal value of $N$, defined as the $argmin$ of $C_p$, vs $n$ (@fig-N_vs_n). The figure shows that the optimal $N$ increases logarithmically as $n$ grows.

Intuitively, as we get more data we can choose narrower buckets that still contain plenty of points and allow to obtain a better fit for the model.

![XXX](optimal_N_vs_n.png){#fig-N_vs_n width=350}




@fig-density is a good way to visualize the result explained above. In this case, we fit the quartic models using different $N$ and $n$. When $n=1,000$, using $N=5$ produces a wiggly result due to having few points in each bucket. But when $n=1,000,000$, this effect is mitigated by the number of points in each bucket, and the fit with $N=5$ is less wiggly.

![XXX](density.png){#fig-density width=500}

## Question 3

![XXX](h_amise_heatmap_n10000_N2.png){width="300"}

### Add a plot of m''(x)